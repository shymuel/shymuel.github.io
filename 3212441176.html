<!DOCTYPE html>
<html lang="zh-CN">
  <head hexo-theme='https://github.com/volantis-x/hexo-theme-volantis/tree/4.3.1'>
  <meta charset="utf-8">
  <!-- SEO相关 -->
  
    
  
  <!-- 渲染优化 -->
  <meta http-equiv='x-dns-prefetch-control' content='on' />
  <link rel='dns-prefetch' href='https://cdn.jsdelivr.net'>
  <link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin>
  <meta name="renderer" content="webkit">
  <meta name="force-rendering" content="webkit">
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
  <meta name="HandheldFriendly" content="True" >
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
  <link rel="preload" href="/css/first.css" as="style">
  

  <!-- 页面元数据 -->
  
  <title>【竹夭的Pytorch讲义】01 - Shymuel&#39;s Blog</title>
  
    <meta name="keywords" content="AI框架,机器学习,Pytorch">
  

  

  <!-- feed -->
  

  <!-- import meta -->
  

  <!-- link -->
  

  <!-- import link -->
  

  
    
<link rel="stylesheet" href="/css/first.css">

  

  
  <link rel="stylesheet" href="/css/style.css" media="print" onload="this.media='all';this.onload=null">
  <noscript><link rel="stylesheet" href="/css/style.css"></noscript>
  

  <script id="loadcss"></script>

  
<script>
if (/*@cc_on!@*/false || (!!window.MSInputMethodContext && !!document.documentMode))
    document.write(
	'<style>'+
		'html{'+
			'overflow-x: hidden !important;'+
			'overflow-y: hidden !important;'+
		'}'+
		'.kill-ie{'+
			'text-align:center;'+
			'height: 100%;'+
			'margin-top: 15%;'+
			'margin-bottom: 5500%;'+
		'}'+
	'</style>'+
    '<div class="kill-ie">'+
        '<h1><b>抱歉，您的浏览器无法访问本站</b></h1>'+
        '<h3>微软已经于2016年终止了对 Internet Explorer (IE) 10 及更早版本的支持，<br/>'+
        '继续使用存在极大的安全隐患，请使用当代主流的浏览器进行访问。</h3><br/>'+
        '<a target="_blank" rel="noopener" href="https://www.microsoft.com/zh-cn/WindowsForBusiness/End-of-IE-support"><strong>了解详情 ></strong></a>'+
    '</div>');
</script>


<noscript>
	<style>
		html{
			overflow-x: hidden !important;
			overflow-y: hidden !important;
		}
		.kill-noscript{
			text-align:center;
			height: 100%;
			margin-top: 15%;
			margin-bottom: 5500%;
		}
	</style>
    <div class="kill-noscript">
        <h1><b>抱歉，您的浏览器无法访问本站</b></h1>
        <h3>本页面需要浏览器支持（启用）JavaScript</h3><br/>
        <a target="_blank" rel="noopener" href="https://www.baidu.com/s?wd=启用JavaScript"><strong>了解详情 ></strong></a>
    </div>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

  <body>
    

<header id="l_header" class="l_header auto shadow blur show" style='opacity: 0' >
  <div class='container'>
  <div id='wrapper'>
    <div class='nav-sub'>
      <p class="title"></p>
      <ul class='switcher nav-list-h m-phone' id="pjax-header-nav-list">
        <li><a id="s-comment" class="fas fa-comments fa-fw" target="_self" href='javascript:void(0)'></a></li>
        
          <li><a id="s-toc" class="s-toc fas fa-list fa-fw" target="_self" href='javascript:void(0)'></a></li>
        
      </ul>
    </div>
		<div class="nav-main">
      
        
        <a class="title flat-box" target="_self" href='/'>
          
            <img no-lazy class='logo' src='https://cdn.jsdelivr.net/gh/volantis-x/cdn-org/blog/Logo-NavBar@3x.png'/>
          
          
          
        </a>
      

			<div class='menu navigation'>
				<ul class='nav-list-h m-pc'>
          
          
          
            
            
              <li>
                <a class="menuitem flat-box faa-parent animated-hover" href=/
                  
                  
                  
                    id="home"
                  >
                  <i class='fas fa-rss fa-fw'></i>博客
                </a>
                
              </li>
            
          
          
            
            
              <li>
                <a class="menuitem flat-box faa-parent animated-hover" href=/categories/
                  
                  
                  
                    id="categories"
                  >
                  <i class='fas fa-folder-open fa-fw'></i>分类
                </a>
                
              </li>
            
          
          
            
            
              <li>
                <a class="menuitem flat-box faa-parent animated-hover" href=/tags/
                  
                  
                  
                    id="tags"
                  >
                  <i class='fas fa-tags fa-fw'></i>标签
                </a>
                
              </li>
            
          
          
            
            
              <li>
                <a class="menuitem flat-box faa-parent animated-hover" href=/archives/
                  
                  
                  
                    id="archives"
                  >
                  <i class='fas fa-archive fa-fw'></i>归档
                </a>
                
              </li>
            
          
          
            
            
              <li>
                <a class="menuitem flat-box faa-parent animated-hover" href=/friends/
                  
                  
                  
                    id="friends"
                  >
                  <i class='fas fa-link fa-fw'></i>友链
                </a>
                
              </li>
            
          
          
            
            
              <li>
                <a class="menuitem flat-box faa-parent animated-hover" href=/about/
                  
                  
                  
                    id="about"
                  >
                  <i class='fas fa-info-circle fa-fw'></i>关于
                </a>
                
              </li>
            
          
          
            
            
              <li>
                <a class="menuitem flat-box header toggle-mode-btn">
                  <i class='fas fa-moon fa-fw'></i>暗黑模式
                </a>
              <li>
            
          
          
				</ul>
			</div>

      <div class="m_search">
        <form name="searchform" class="form u-search-form">
          <i class="icon fas fa-search fa-fw"></i>
          <input type="text" class="input u-search-input" placeholder="Search..." />
        </form>
      </div>

			<ul class='switcher nav-list-h m-phone'>
				
					<li><a class="s-search fas fa-search fa-fw" target="_self" href='javascript:void(0)'></a></li>
				
				<li>
          <a class="s-menu fas fa-bars fa-fw" target="_self" href='javascript:void(0)'></a>
          <ul class="menu-phone list-v navigation white-box">
            
              
            
              <li>
                <a class="menuitem flat-box faa-parent animated-hover" href=/
                  
                  
                  
                    id="home"
                  >
                  <i class='fas fa-rss fa-fw'></i>博客
                </a>
                
              </li>
            
          
            
              
            
              <li>
                <a class="menuitem flat-box faa-parent animated-hover" href=/categories/
                  
                  
                  
                    id="categories"
                  >
                  <i class='fas fa-folder-open fa-fw'></i>分类
                </a>
                
              </li>
            
          
            
              
            
              <li>
                <a class="menuitem flat-box faa-parent animated-hover" href=/tags/
                  
                  
                  
                    id="tags"
                  >
                  <i class='fas fa-tags fa-fw'></i>标签
                </a>
                
              </li>
            
          
            
              
            
              <li>
                <a class="menuitem flat-box faa-parent animated-hover" href=/archives/
                  
                  
                  
                    id="archives"
                  >
                  <i class='fas fa-archive fa-fw'></i>归档
                </a>
                
              </li>
            
          
            
              
            
              <li>
                <a class="menuitem flat-box faa-parent animated-hover" href=/friends/
                  
                  
                  
                    id="friends"
                  >
                  <i class='fas fa-link fa-fw'></i>友链
                </a>
                
              </li>
            
          
            
              
            
              <li>
                <a class="menuitem flat-box faa-parent animated-hover" href=/about/
                  
                  
                  
                    id="about"
                  >
                  <i class='fas fa-info-circle fa-fw'></i>关于
                </a>
                
              </li>
            
          
            
              
            
              <li>
                <a class="menuitem flat-box header toggle-mode-btn">
                  <i class='fas fa-moon fa-fw'></i>暗黑模式
                </a>
              <li>
            
          
            
          </ul>
        </li>
			</ul>
		</div>
	</div>
  </div>
</header>

    <div id="l_body">
      <div id="l_cover">
  
    
        <div id="full" class='cover-wrapper post search' style="display: none;">
          
            <div class='cover-bg lazyload placeholder' data-bg="/img/background_1.jpg"></div>
          
          <div class='cover-body'>
  <div class='top'>
    
    
      <p class="title">Shymuel's Blog</p>
    
    
  </div>
  <div class='bottom'>
    
      <div class="m_search">
        <form name="searchform" class="form u-search-form">
          <input type="text" class="input u-search-input" placeholder="Here may be something useful." />
          <i class="icon fas fa-search fa-fw"></i>
        </form>
      </div>
    
    <div class='menu navigation'>
      <div class='list-h'>
        
          
            <a href="/"
              
              
              id="home">
              <p>首页</p>
            </a>
          
            <a href="/archives/"
              
              
              id="archives">
              <p>时间轴</p>
            </a>
          
            <a href="/friends/"
              
              
              id="friends">
              <p>友链</p>
            </a>
          
        
      </div>
    </div>
  </div>
</div>

          <div id="scroll-down" style="display: none;"><i class="fa fa-chevron-down scroll-down-effects"></i></div>
        </div>
    
  
  </div>

      <div id="safearea">
        <div class="body-wrapper" id="pjax-container">
          

<div class='l_main'>
  <article class="article post white-box reveal md shadow article-type-post" id="post" itemscope itemprop="blogPost">
  


  
  <div class="article-meta" id="top">
    
    
    
      <h1 class="title">
        【竹夭的Pytorch讲义】01
      </h1>
      <div class='new-meta-box'>
        
          
            
<div class='new-meta-item author'>
  <a class='author' href="/" rel="nofollow">
    <img no-lazy src="/img/avatar.jpg">
    <p>Shymuel</p>
  </a>
</div>

          
        
          
            
  <div class='new-meta-item category'>
    <a class='notlink'>
      <i class="fas fa-folder-open fa-fw" aria-hidden="true"></i>
      <a class="category-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
    </a>
  </div>


          
        
          
            <div class="new-meta-item date">
  <a class='notlink'>
    <i class="fas fa-calendar-alt fa-fw" aria-hidden="true"></i>
    <p>发布于：2024年1月20日</p>
  </a>
</div>

          
        
          
            
  <div class="new-meta-item browse leancloud">
    <a class='notlink'>
      
      <div id="lc-pv" data-title="【竹夭的Pytorch讲义】01" data-path="/3212441176.html">
        <i class="fas fa-eye fa-fw" aria-hidden="true"></i>
        <span id='number'><i class="fas fa-circle-notch fa-spin fa-fw" aria-hidden="true"></i></span>
        次浏览
      </div>
    </a>
  </div>


          
        
          
            
  <div class="new-meta-item wordcount">
    <a class='notlink'>
      <i class="fas fa-keyboard fa-fw" aria-hidden="true"></i>
      <p>字数：6.4k字</p>
    </a>
  </div>
  <div class="new-meta-item readtime">
    <a class='notlink'>
      <i class="fas fa-hourglass-half fa-fw" aria-hidden="true"></i>
      <p>时长：25分钟</p>
    </a>
  </div>


          
        
      </div>
    
  </div>


  
  
  <h1 id="大纲">0 大纲</h1>
<ol type="1">
<li>安装和设置环境
<ul>
<li>配置开发环境，如安装适当的Python版本和IDE，安装PyTorch和相关的软件包</li>
<li>初识pytorch</li>
</ul></li>
<li>张量和张量操作
<ul>
<li>张量的概念和基本操作</li>
<li>张量的创建、索引和切片操作</li>
<li>张量的数学运算和广播操作</li>
</ul></li>
<li>自动求导和反向传播
<ul>
<li>自动求导的概念和原理</li>
<li>使用PyTorch进行自动求导和反向传播</li>
<li>优化器和学习率</li>
</ul></li>
<li>构建神经网络模型
<ul>
<li>PyTorch中的神经网络模型的组成和结构</li>
<li>构建自定义的神经网络模型</li>
<li>常见的神经网络层和激活函数</li>
</ul></li>
<li>数据加载和预处理
<ul>
<li>数据集的收集和准备</li>
<li>使用PyTorch的<code>Dataset</code>和<code>DataLoader</code>加载和处理数据</li>
<li>数据增强和预处理操作</li>
</ul></li>
<li>训练和评估模型
<ul>
<li>训练模型的基本流程和步骤</li>
<li>定义损失函数和评估指标</li>
<li>批量训练和迭代优化模型</li>
<li>模型的评估和性能指标的计算</li>
</ul></li>
<li>迁移学习和预训练模型
<ul>
<li>迁移学习的概念和应用</li>
<li>使用预训练模型进行迁移学习的步骤</li>
<li>常见的预训练模型和使用方法</li>
</ul></li>
<li>模型部署和推理
<ul>
<li>将训练好的模型导出和保存</li>
<li>在生产环境中使用模型进行推理和预测</li>
<li>常见的模型部署方法和工具</li>
</ul></li>
</ol>
<h1 id="安装和设置环境">1 安装和设置环境</h1>
<h2 id="配置开发环境">1.1 配置开发环境</h2>
<p>用anaconda3管理包，用pycharm（学生可以免费用professional版）编写代码。</p>
<p>安装pytorch不需要翻墙，直接用官网的命令即可。</p>
<h2 id="初识pytorch">1.2 初识pytorch</h2>
<p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/1.11/">pytorch官方文档</a>。</p>
<h3 id="两个实用函数">1.2.1 两个实用函数</h3>
<p>研究Pytorch包之类的新package用到两个函数：<code>dir()</code>，用来打开package，输出分割区；<code>help()</code>，用来输出package或其中一些内容的帮助文档，一般看函数的帮助文档，注意函数后面不需要加括号。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">ans = torch.cuda.is_available()</span><br><span class="line"><span class="built_in">print</span>(ans)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">dir</span>(torch))</span><br><span class="line"><span class="built_in">help</span>(torch)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">dir</span>(torch.AnyType))</span><br><span class="line"><span class="built_in">help</span>(torch.cuda.is_available)</span><br></pre></td></tr></table></figure>
<p>可以运行上述代码，观察输出。</p>
<p>对pytorch的初步认识：</p>
<p>Pytorch实现模型训练：数据+模型+损失函数+优化器=迭代训练。</p>
<ul>
<li>数据：包括数据读取和数据清洗等。</li>
<li>模型：包括构建模型模块，组织复杂网络，初始化网络参数，定义网络层。</li>
<li>损失函数：包括创建损失函数，设置损失函数超参数，根据不同任务选择合适的损失函数。</li>
<li>优化器：包括根据梯度使用某种优化器更新参数，管理模型参数，管理多个参数组实现不同学习率，调整学习率。</li>
<li>迭代训练：组织上面4个模块进行反复训练。包括观察训练效果，绘制Loss/Accuracy曲线，用TensorBoard进行可视化分析。</li>
</ul>
<h1 id="张量和张量操作">2 张量和张量操作</h1>
<h2 id="张量的概念和基本操作">2.1 张量的概念和基本操作</h2>
<p>本章代码：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/zhangxiann/PyTorch_Practice/blob/master/lesson1/tensor_introduce1.py">https://github.com/zhangxiann/PyTorch_Practice/blob/master/lesson1/tensor_introduce1.py</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/zhangxiann/PyTorch_Practice/blob/master/lesson1/tensor_introduce1.py">https://github.com/zhangxiann/PyTorch_Practice/blob/master/lesson1/tensor_introduce1.py</a></li>
</ul>
<p>标量可以称为0维张量，向量可以称为1维张量，矩阵可以称为2维张量，RGB图像可以表示3维张量。可以把张量看作多维数组。</p>
<p><strong>size和shape的关系</strong></p>
<p>Numpy中，size()和shape()是两个函数，np.size(a)返回a中所有元素的个数，np.shape(a)返回a每维的大小。</p>
<p>Pytorch中，shape是张量的一个属性，比如a.shape。a.size会输出a在内存中的占用情况，a.size(axis)会输出a在axis维度上的大小。</p>
<p><strong>Tensor与Variable</strong></p>
<p>在PyTorch 0.4.0之前，torch.autograd包中存在Variable这种数据类型，主要是用于封装Tensor，进行自动求导。Variable主要包含下面几种属性。</p>
<ul>
<li>data：被包装的Tensor。</li>
<li>grad：data 的梯度。</li>
<li>grad_fn：创建 Tensor 所使用的 Function，是自动求导的关键，根据所记录的函数才能计算出导数。</li>
<li>requires_grad：指示是否需要梯度，并不是所有的张量都需要计算梯度。</li>
<li>is_leaf：指示是否叶子节点(张量)，叶子节点的概念在计算图中会用到，后面详细介绍。</li>
</ul>
<figure>
<img src="\img\PytorchMaterial01\zYh4LIVVFoKV7WIW2Rlr7CGSk9Zx_XiqVr-BOVwL440.png" class="lazyload" data-srcset="\img\PytorchMaterial01\zYh4LIVVFoKV7WIW2Rlr7CGSk9Zx_XiqVr-BOVwL440.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="Variable类的属性" /><figcaption aria-hidden="true">Variable类的属性</figcaption>
</figure>
<p>在PyTorch 0.4.0之后，Variable并入了Tensor。在之后版本的Tensor中，除了具有上面Variable的5个属性，还有另外3个属性。</p>
<ul>
<li>dtype：张量的数据类型，如torch.FloatTensor，torch.cuda.FloatTensor。</li>
<li>shape：张量的形状，如(64, 3, 224, 224)。</li>
<li>device：张量所在设备 (CPU/GPU)，GPU是加速计算的关键。</li>
</ul>
<figure>
<img src="\img\PytorchMaterial01\w8VfyAXy7mCS6furVknGmccs3Tm7BYnkbxi0ZQFInbU.png" class="lazyload" data-srcset="\img\PytorchMaterial01\w8VfyAXy7mCS6furVknGmccs3Tm7BYnkbxi0ZQFInbU.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="Tensor类的属性" /><figcaption aria-hidden="true">Tensor类的属性</figcaption>
</figure>
<p>关于dtype，PyTorch提供了9种数据类型，共分为3大类：float(16-bit, 32-bit, 64-bit)、integer(unsigned-8-bit ,8-bit, 16-bit, 32-bit, 64-bit)、Boolean。模型参数和数据用的最多的类型是float-32-bit。label 常用的类型是integer-64-bit。</p>
<figure>
<img src="\img\PytorchMaterial01\N0lkHqYEFcF326qIJfskWNFNPWPx4a25ajWS9JYLao4.png" class="lazyload" data-srcset="\img\PytorchMaterial01\N0lkHqYEFcF326qIJfskWNFNPWPx4a25ajWS9JYLao4.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" alt="常用数据类型及其写法" /><figcaption aria-hidden="true">常用数据类型及其写法</figcaption>
</figure>
<h2 id="张量的创建和索引">2.2 张量的创建和索引</h2>
<h3 id="张量的创建">2.2.1 张量的创建</h3>
<p><strong>直接创建Tensor</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor(data, dtype=<span class="literal">None</span>, device=<span class="literal">None</span>, requires_grad=<span class="literal">False</span>, pin_memory=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>data：数据，可以是list，numpy。</li>
<li>dtype：数据类型，默认与data的一致。</li>
<li>device：所在设备，cuda/cpu。</li>
<li>requires_grad：是否需要梯度。</li>
<li>pin_memory：是否存于锁页内存。</li>
</ul>
<p><strong>从numpy创建Tensor</strong></p>
<p>使用<code>torch.from_numpy(ndarray)</code> 。利用这个方法创建的tensor和原来的ndarray共享内存，当修改其中一个数据，另外一个也会被改动。</p>
<p><strong>根据数值创建Tensor</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.zeros(*size, out=<span class="literal">None</span>, dtype=<span class="literal">None</span>, layout=torch.strided, device=<span class="literal">None</span>, requires_grad=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>根据size创建全0张量。</p>
<ul>
<li>size：张量的形状。</li>
<li>out：输出的张量。如果指定了out，那么<code>torch.zeros()</code>返回的张量和 out 指向的是同一个地址。</li>
<li>layout：内存中布局形式，有strided，sparse_coo等。当是稀疏矩阵时，设置为sparse_coo可以减少内存占用。</li>
<li>device：所在设备，cuda/cpu。</li>
<li>requires_grad：是否需要梯度。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.zeros_like(<span class="built_in">input</span>, dtype=<span class="literal">None</span>, layout=<span class="literal">None</span>, device=<span class="literal">None</span>, requires_grad=<span class="literal">False</span>, memory_format=torch.preserve_format)</span><br></pre></td></tr></table></figure>
<p>根据input的形状创建全0张量。</p>
<ul>
<li>input：创建与input同形状的全0张量。</li>
<li>dtype：数据类型。</li>
<li>layout：内存中布局形式，有strided，sparse_coo等。当是稀疏矩阵时，设置为sparse_coo可以减少内存占用。</li>
</ul>
<p>同理还有全1张量的创建方法：<code>torch.ones()</code>，<code>torch.ones_like()</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.full(size, fill_value, out=<span class="literal">None</span>, dtype=<span class="literal">None</span>, layout=torch.strided, device=<span class="literal">None</span>, requires_grad=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p><code>torch.full()</code>，<code>torch.full_like()</code>可以创建自定义数值的张量。</p>
<ul>
<li>size：张量的形状，如(3,3)。</li>
<li>fill_value：张量中每一个元素的值。</li>
</ul>
<p><strong>根据数学公式创建Tensor</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.arange(start=<span class="number">0</span>, end, step=<span class="number">1</span>, out=<span class="literal">None</span>, dtype=<span class="literal">None</span>, layout=torch.strided, device=<span class="literal">None</span>, requires_grad=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>创建等差的1维张量。注意区间为[start, end)。</p>
<ul>
<li>start：数列起始值。</li>
<li>end：数列结束值，开区间，取不到结束值。</li>
<li>step：数列公差，默认为1。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.linspace(start, end, steps=<span class="number">100</span>, out=<span class="literal">None</span>, dtype=<span class="literal">None</span>, layout=torch.strided, device=<span class="literal">None</span>, requires_grad=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>创建均分的1维张量。数值区间为[start, end]。<span class="math inline">\(a_i=a_0+\frac{end-start}{step-1}*i\)</span></p>
<ul>
<li>start：数列起始值。</li>
<li>end：数列结束值。</li>
<li>steps：数列长度，即元素个数。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.logspace(start, end, steps=<span class="number">100</span>, base=<span class="number">10.0</span>, out=<span class="literal">None</span>, dtype=<span class="literal">None</span>, layout=torch.strided, device=<span class="literal">None</span>, requires_grad=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>创建对数均分的1维张量。数值区间为[start, end]，底为base。</p>
<ul>
<li>start：数列起始值。</li>
<li>end：数列结束值。</li>
<li>steps：数列长度（元素个数）。</li>
<li>base：对数函数的底，默认为10。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.eye(n, m=<span class="literal">None</span>, out=<span class="literal">None</span>, dtype=<span class="literal">None</span>, layout=torch.strided, device=<span class="literal">None</span>, requires_grad=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>创建单位对角矩阵（2维张量），默认为方阵。</p>
<ul>
<li>n：矩阵行数。通常只设置n，为方阵。</li>
<li>m：矩阵列数。</li>
</ul>
<p><strong>根据概率创建Tensor</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.normal(mean, std, *, generator=<span class="literal">None</span>, out=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<p>生成正态分布（高斯分布）。</p>
<ul>
<li>mean：均值。</li>
<li>std：标准差。</li>
</ul>
<p>该函数有4种模式：</p>
<ol type="1">
<li>mean为标量，std为标量。这时需要设置size。</li>
<li>mean为标量，std为张量。</li>
<li>mean为张量，std为标量。</li>
<li>mean为张量，std为张量。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.randn(*size, out=<span class="literal">None</span>, dtype=<span class="literal">None</span>, layout=torch.strided, device=<span class="literal">None</span>, requires_grad=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p><code>torch.randn()</code>和<code>torch.randn_like()</code> ，用来生成标准正态分布。</p>
<ul>
<li>size：张量的形状。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.rand(*size, out=<span class="literal">None</span>, dtype=<span class="literal">None</span>, layout=torch.strided, device=<span class="literal">None</span>, requires_grad=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p><code>torch.rand()</code>和<code>torch.rand_like()</code>在区间[0, 1)上生成均匀分布。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">randint(low=<span class="number">0</span>, high, size, *, generator=<span class="literal">None</span>, out=<span class="literal">None</span>,</span><br><span class="line">dtype=<span class="literal">None</span>, layout=torch.strided, device=<span class="literal">None</span>, requires_grad=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p><code>torch.randint()</code>和<code>torch.randint_like()</code> ，在区间[low, high)上生成整数均匀分布并采样。</p>
<ul>
<li>size：张量的形状。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.randperm(n, out=<span class="literal">None</span>, dtype=torch.int64, layout=torch.strided, device=<span class="literal">None</span>, requires_grad=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>生成从0到n-1的随机排列，常用于生成索引。</p>
<ul>
<li>n：张量的长度。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.bernoulli(<span class="built_in">input</span>, *, generator=<span class="literal">None</span>, out=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<p>以input为概率，生成伯努利分布（即0-1 分布，两点分布）。</p>
<ul>
<li>input：概率值。</li>
</ul>
<h3 id="张量的索引">2.2.2 张量的索引</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.<span class="built_in">max</span>(<span class="built_in">input</span>, dim, keepdim=<span class="literal">False</span>, out=<span class="literal">None</span>) -&gt; (Tensor, LongTensor)</span><br></pre></td></tr></table></figure>
<p>按维度dim返回最大值以及最大值的索引。dim = 0表示按列求最大值，dim = 1表示按行求最大值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nonzero(<span class="built_in">input</span>)</span><br></pre></td></tr></table></figure>
<p>返回一个包含输入input中非零元素索引的张量，输出张量中的每行包含input中非零元素的索引。如果输入input有n维，则输出的索引张量out的size为z×n , 这里z是输入张量input中所有非零元素的个数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.where(condition，a，b)</span><br></pre></td></tr></table></figure>
<p>按照一定的规则合并两个tensor类型。</p>
<p>condition：条件限制，如果满足条件，则选择a，否则选择b作为输出。注意a和b是tensor。</p>
<p>numpy中也有where()，其输入非tensor。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.index_select(<span class="built_in">input</span>, dim, index, out=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<p>在维度dim上，按照index索引取出数据拼接为张量返回。</p>
<ul>
<li>input：要索引的张量。</li>
<li>dim：要索引的维度。</li>
<li>index：要索引数据的序号。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.masked_select(<span class="built_in">input</span>, mask, out=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<p>按照mask中的True进行索引拼接得到一维张量返回。</p>
<ul>
<li>input：要索引的张量。</li>
<li>mask：与input同形状的布尔类型张量。</li>
</ul>
<h2 id="张量的维度操作和数学运算">2.3 张量的维度操作和数学运算</h2>
<h3 id="张量的维度操作">2.3.1 张量的维度操作</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.cat(tensors, dim=<span class="number">0</span>, out=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<p>将张量按照dim维度进行拼接。</p>
<ul>
<li>tensors：张量序列。</li>
<li>dim：要拼接的维度。dim=0是行数，1是列数。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.stack(tensors, dim=<span class="number">0</span>, out=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<p>将张量在新创建的dim维度上进行拼接。</p>
<ul>
<li>tensors：张量序列。</li>
<li>dim：要拼接的维度。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.chunk(<span class="built_in">input</span>, chunks, dim=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>将张量按照维度dim进行平均切分。若不能整除，则最后一份张量小于其他张量。</p>
<ul>
<li>input：要切分的张量。</li>
<li>chunks：要切分的份数。</li>
<li>dim：要切分的维度。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.split(tensor, split_size_or_sections, dim=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>将张量按照维度dim进行平均切分。可以指定每一个分量的切分长度。</p>
<ul>
<li>tensor：要切分的张量</li>
<li>split_size_or_sections：为int时表示每一份的长度，如果不能被整除，则最后一份张量小于其他张量；为list时按照list元素作为每一个分量的长度切分。如果list元素之和不等于切分维度dim的值则报错。</li>
<li>dim：要切分的维度。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.reshape(<span class="built_in">input</span>, shape)</span><br></pre></td></tr></table></figure>
<p>变换张量的形状（维度）。当张量在内存中是连续时，返回的张量和原来的张量共享数据内存，改变一个变量时，另一个变量也会被改变。</p>
<ul>
<li>input：要变换的张量。</li>
<li>shape：新张量的形状。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.transpose(<span class="built_in">input</span>, dim0, dim1)</span><br></pre></td></tr></table></figure>
<p>交换张量的两个维度。常用于图像的变换，比如把<code>c*h*w</code>变换为<code>c*w*h</code>。</p>
<ul>
<li>input：要交换维度的张量。</li>
<li>dim0：要交换的第一个维度。</li>
<li>dim1：要交换的第二个维度。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.t(<span class="built_in">input</span>)</span><br></pre></td></tr></table></figure>
<p>2维张量转置。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.squeeze(<span class="built_in">input</span>, dim=<span class="literal">None</span>, out=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<p>压缩长度为1的维度。</p>
<ul>
<li>dim：若为None，则移除所有长度为1的维度；若指定维度，则当且仅当该维度长度为1时可以移除。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.unsqueeze(<span class="built_in">input</span>, dim)</span><br></pre></td></tr></table></figure>
<p>根据dim扩展维度，长度为1。</p>
<h3 id="张量的数学运算">2.3.2 张量的数学运算</h3>
<p>主要分为3类：加减乘除，对数指数，幂函数和三角函数。这里介绍一下常用的几种方法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.add(<span class="built_in">input</span>, other, out=<span class="literal">None</span>)</span><br><span class="line">torch.add(<span class="built_in">input</span>, other, *, alpha=<span class="number">1</span>, out=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<p>逐元素计算input+alpha*other。因为在深度学习中经常用到先乘后加的操作。</p>
<ul>
<li>input：第一个张量。</li>
<li>alpha：乘项因子。</li>
<li>other：第二个张量。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.addcdiv(<span class="built_in">input</span>, tensor1, tensor2, *, value=<span class="number">1</span>, out=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<p><span class="math inline">\(\text{out}_{i}=\text{input}_{i}+\text{value}\times \frac{\text{tensor1}_{i}}{\text{tensor2}_{i}}\)</span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.addcmul(<span class="built_in">input</span>, tensor1, tensor2, *, value=<span class="number">1</span>, out=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<p><span class="math inline">\(\text{out}_{i}=\text{input}_{i}+\text{value}\times \text{tensor1}_{i} \times \text{tensor 2}_{i}\)</span></p>
<h2 id="section"></h2>
<h1 id="自动求导和反向传播">3 自动求导和反向传播</h1>
<h2 id="计算图与自动求导">3.1 计算图与自动求导</h2>
<p><strong>计算图</strong></p>
<p>计算图是用来描述运算的有向无环图，主要包括节点（Node）和边（Edge）。节点表示数据，如向量、矩阵、张量。边表示运算，如加减乘除卷积等。</p>
<p>有公式<span class="math inline">\(y=(x+w)*(w+1)\)</span>，<span class="math inline">\(x=2, \ w=1\)</span>，对<span class="math inline">\(w\)</span>求导，根据复合函数的求导法则可以得到如下过程：</p>
<p><span class="math inline">\(\begin{aligned} \frac{\partial y}{\partial w} &amp;=\frac{\partial y}{\partial a} \frac{\partial a}{\partial w}+\frac{\partial y}{\partial b} \frac{\partial b}{\partial w} \\ &amp;=b\times1+a\times1 \\ &amp;=b+a \\ &amp;=(w+1)+(x+w) \\ &amp;=2 w+x+1 \\ &amp;=2\times1+2+1=5\end{aligned}\)</span></p>
<p>前面说过Tensor中有一个属性<code>is_leaf</code>标记是否为叶子节点，这个概念主要是为了节省内存，在计算图中的一轮反向传播结束之后，非叶子节点的梯度是会被释放的。如果在反向传播结束之后仍然需要保留非叶子节点的梯度，可以使用张量的<code>retain_grad()</code>方法：<code>a.retain_grad()</code>。</p>
<p>Tensor中的<code>grad_fn</code>属性记录的是创建该张量时所用的方法（函数），反向传播求导梯度时需要用到该属性。</p>
<p><strong>PyTorch的动态图机制</strong></p>
<p>PyTorch采用的是动态图机制（Dynamic Computational Graph），而Tensorflow采用的是静态图机制（Static Computational Graph）。动态图是运算和搭建同时进行，也就是可以先计算前面的节点的值再根据这些值搭建后面的计算图。优点是灵活，易调节易调试。PyTorch的写法跟其他Python库是完全一致的，没有额外的学习成本。</p>
<p>静态图是先搭建图，然后再输入数据进行运算。优点是高效，因为静态计算是通过先定义后运行的方式，之后再次运行的时候就不再需要重新构建计算图，所以速度会比动态图更快。但是不灵活。TensorFlow每次运行的时候图都是一样的，是不能够改变的，所以不能直接使用Python的while循环语句，需要使用辅助函数tf.while_loop写成TensorFlow内部的形式。</p>
<p><strong>自动求导（autograd）</strong></p>
<p>深度学习中，权值的更新依赖于梯度的计算，因此梯度的计算是至关重要的。PyTorch 中只需要搭建好前向计算图，就可以利用<code>torch.autograd</code>自动求导得到所有张量的梯度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.autograd.backward(tensors, grad_tensors=<span class="literal">None</span>, retain_graph=<span class="literal">None</span>, create_graph=<span class="literal">False</span>, grad_variables=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<p>自动求取梯度。</p>
<ul>
<li>tensors：用于求导的张量，如loss。</li>
<li>retain_graph：保存计算图。PyTorch采用动态图机制，默认每次反向传播之后都会释放计算图。该参数设置为True可以不释放计算图，这样就可以多次计算导数。</li>
<li>create_graph：创建导数计算图，用于高阶求导。</li>
<li>grad_tensors：多梯度权重。当有多个loss混合计算梯度时，设置每个loss的权重。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.autograd.grad(outputs, inputs, grad_outputs=<span class="literal">None</span>, retain_graph=<span class="literal">None</span>, create_graph=<span class="literal">False</span>, only_inputs=<span class="literal">True</span>, allow_unused=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>求取梯度。</p>
<ul>
<li>outputs：用于求导的张量，如loss。</li>
<li>inputs：需要梯度的张量。</li>
<li>create_graph：创建导数计算图，用于高阶求导。</li>
<li>retain_graph：保存计算图。</li>
<li>grad_outputs：多梯度权重计算。</li>
</ul>
<p><code>torch.autograd.grad()</code>的返回结果是一个 tunple，需要取出第0个元素才是真正的梯度。</p>
<p>求一阶导时需要设置<code>create_graph=True</code>，让一阶导数<code>grad_1</code>也拥有计算图，然后再使用一阶导求取二阶导。</p>
<p>需要注意的3个点：</p>
<ul>
<li><p>在每次反向传播求导时，计算的梯度不会自动清零。如果进行多次迭代计算梯度而没有清零，那么梯度会在前一次的基础上叠加。使用<code>w.grad.zero_()</code>可将梯度清零。</p></li>
<li><p>依赖于叶子节点的节点，requires_grad属性默认为True。</p></li>
<li><p>叶子节点不可执行inplace操作。以加法来说，inplace操作有<code>a += x</code>，<code>a.add_(x)</code>，改变后的值和原来的值内存地址是同一个。非inplace操作有<code>a = a + x</code>，<code>a.add(x)</code>，改变后的值和原来的值内存地址不是同一个。</p></li>
</ul>
<p>如果在反向传播之前用inplace方法改变了叶子节点的值，再执行backward()会报错。这是因为在进行前向传播时，计算图中依赖于叶子节点的那些节点会记录叶子节点的地址，在反向传播时就会利用叶子节点的地址所记录的值来计算梯度。比如在<span class="math inline">\(y=a \times b\)</span>，其中<span class="math inline">\(a=x+w\)</span>，<span class="math inline">\(b=w+1\)</span>，<span class="math inline">\(x\)</span>和<span class="math inline">\(w\)</span>是叶子节点。当求导<span class="math inline">\(\frac{\partial y}{\partial a} = b = w+1\)</span>，需要用到叶子节点<span class="math inline">\(w\)</span>。</p>
<h2 id="使用pytorch进行自动求导和反向传播">3.2 使用PyTorch进行自动求导和反向传播</h2>
<p>以<strong>逻辑回归（Logistic Regression）</strong>为例，基于PyTorch实现自动求导和反向传播。</p>
<p>逻辑回归是线性的二分类模型。模型表达式<span class="math inline">\(y=f(z)=\frac{1}{1+e^{-z}}\)</span>，其中<span class="math inline">\(z=WX+b\)</span>。<span class="math inline">\(f(z)\)</span>称为sigmoid函数，也被称为Logistic函数。</p>
<p><span class="math inline">\(y&lt;0.5\)</span>时类别为0；<span class="math inline">\(0.5 \leq y\)</span>时类别为1。</p>
<p>其中<span class="math inline">\(z=WX+b\)</span>也是线性回归的模型。从横坐标来看，当<span class="math inline">\(z&lt;0\)</span>时，类别为0；当<span class="math inline">\(0 \leq z\)</span> 时，类别为1，直接使用线性回归也可以进行分类。逻辑回归是在线性回归的基础上加入了一个sigmoid 函数，这是为了更好地描述置信度，把输入映射到(0,1)区间中，符合概率取值。</p>
<p>逻辑回归也被称为对数几率回归：<span class="math inline">\(\ln \frac{y}{1-y}=W X+b\)</span>。几率的表达式为：<span class="math inline">\(\frac{y}{1-y}\)</span>，<span class="math inline">\(y\)</span>表示正类别的概率，<span class="math inline">\(1-y\)</span>表示另一个类别的概率。根据对数几率回归可以推导出逻辑回归表达式：</p>
<p><span class="math inline">\(\begin{aligned}&amp;\ln \frac{y}{1-y}=W X+b \\&amp;\frac{y}{1-y}=e^{W X+b} \\&amp;y=e^{W X+b}-y * e^{W X+b} \\&amp;y\left(1+e^{W X+b}\right)=e^{W X+b} \\&amp;y=\frac{e^{W X+b}}{1+e^{W X+b}}=\frac{1}{1+e^{-(W X+b)}}\end{aligned}\)</span></p>
<p><strong>Pytorch实现逻辑回归</strong></p>
<p>PyTorch 构建模型需要5大步骤：</p>
<ul>
<li>数据：进行数据预处理。</li>
<li>模型：包括构建模型模块，组织复杂网络，初始化网络参数，定义网络层。</li>
<li>损失函数：包括创建损失函数，设置损失函数超参数，根据不同任务选择合适的损失函数。</li>
<li>优化器：包括根据梯度使用某种优化器更新参数，管理模型参数，管理多个参数组实现不同学习率，调整学习率。</li>
<li>迭代训练：组织上面4个模块进行反复训练。包括观察训练效果，绘制Loss/Accuracy曲线，用TensorBoard进行可视化分析。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Linear</span>(<span class="params">Module</span>):</span></span><br><span class="line">    <span class="string">r&quot;&quot;&quot;Applies a linear transformation to the incoming data: :math:`y = xA^T + b`</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        in_features: size of each input sample</span></span><br><span class="line"><span class="string">        out_features: size of each output sample</span></span><br><span class="line"><span class="string">        bias: If set to ``False``, the layer will not learn an additive bias.</span></span><br><span class="line"><span class="string">            Default: ``True``</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Shape:</span></span><br><span class="line"><span class="string">        - Input: :math:`(*, H_&#123;in&#125;)` where :math:`*` means any number of</span></span><br><span class="line"><span class="string">          dimensions including none and :math:`H_&#123;in&#125; = \text&#123;in\_features&#125;`.</span></span><br><span class="line"><span class="string">        - Output: :math:`(*, H_&#123;out&#125;)` where all but the last dimension</span></span><br><span class="line"><span class="string">          are the same shape as the input and :math:`H_&#123;out&#125; = \text&#123;out\_features&#125;`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Attributes:</span></span><br><span class="line"><span class="string">        weight: the learnable weights of the module of shape</span></span><br><span class="line"><span class="string">            :math:`(\text&#123;out\_features&#125;, \text&#123;in\_features&#125;)`. The values are</span></span><br><span class="line"><span class="string">            initialized from :math:`\mathcal&#123;U&#125;(-\sqrt&#123;k&#125;, \sqrt&#123;k&#125;)`, where</span></span><br><span class="line"><span class="string">            :math:`k = \frac&#123;1&#125;&#123;\text&#123;in\_features&#125;&#125;`</span></span><br><span class="line"><span class="string">        bias:   the learnable bias of the module of shape :math:`(\text&#123;out\_features&#125;)`.</span></span><br><span class="line"><span class="string">                If :attr:`bias` is ``True``, the values are initialized from</span></span><br><span class="line"><span class="string">                :math:`\mathcal&#123;U&#125;(-\sqrt&#123;k&#125;, \sqrt&#123;k&#125;)` where</span></span><br><span class="line"><span class="string">                :math:`k = \frac&#123;1&#125;&#123;\text&#123;in\_features&#125;&#125;`</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Examples::</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; m = nn.Linear(20, 30)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; input = torch.randn(128, 20)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; output = m(input)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; print(output.size())</span></span><br><span class="line"><span class="string">        torch.Size([128, 30])</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nn.BCEloss()  <span class="comment"># 计算二分类问题的交叉熵</span></span><br></pre></td></tr></table></figure>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/zhangxianrong/p/14773075.html">交叉熵介绍博客</a>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.optim.SGD(params, lr, momentum, dampening, weight_decay, nesterov)</span><br></pre></td></tr></table></figure>
<ul>
<li>params：待优化参数的iterable，或者是定义了参数组的dict。这个参数代表权重<span class="math inline">\(w\)</span>和偏置<span class="math inline">\(b\)</span>等神经网络中的参数。</li>
<li>lr：学习率。</li>
<li>momentum：动量因子。每次<span class="math inline">\(x\)</span>的更新量<span class="math inline">\(v=-dx*lr\)</span>，考虑动量之后公式变为<span class="math inline">\(v&#39;=-dx*lr+v*momenmtum\)</span>。</li>
<li>weight_decay：权重惩罚。</li>
<li>dampening：动量的抑制因子。</li>
<li>nesterov：使用nesterov动量。</li>
</ul>
<p>用<code>.item()</code>取出张量中的元素而不是用下标，主要是精度有区别。</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/ys1305/article/details/97959046/">Pytorch各层输出</a>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">matplotlib.pyplot.scatter(x, y, s=<span class="literal">None</span>, c=<span class="literal">None</span>, marker=<span class="literal">None</span>, cmap=<span class="literal">None</span>, norm=<span class="literal">None</span>, vmin=<span class="literal">None</span>, vmax=<span class="literal">None</span>, alpha=<span class="literal">None</span>, linewidths=<span class="literal">None</span>, verts=<span class="literal">None</span>, edgecolors=<span class="literal">None</span>, *, data=<span class="literal">None</span>, **kwargs)</span><br></pre></td></tr></table></figure>
<p>用于生成一个scatter散点图。</p>
<ul>
<li>x, y：表示的是shape大小为(n,)的数组，也就是我们即将绘制散点图的数据点，输入数据。</li>
<li>s：表示的是散点图上一个点的大小，是一个标量或者是一个shape大小为(n,)的数组，可选，默认20。</li>
<li>c：表示的是色彩或颜色序列，可选，默认蓝色'b'。但是c不应该是一个单一的RGB数字，也不应该是一个RGBA的序列，因为不便区分。c可以是一个RGB或RGBA二维行数组。</li>
<li>marker：MarkerStyle，表示的是标记的样式，可选，默认'o'。</li>
<li>cmap：Colormap，标量或者是一个colormap的名字，cmap仅仅当c是一个浮点数数组的时候才使用。如果没有申明就是image.cmap，可选，默认None。</li>
<li>norm：Normalize，数据亮度在0-1之间，也是只有c是一个浮点数的数组的时候才使用。如果没有申明，就是默认None。</li>
<li>vmin，vmax：标量，当norm存在的时候忽略。用来进行亮度数据的归一化，可选，默认None。</li>
<li>alpha：散点的透明度。标量，0-1之间，可选，默认None。</li>
<li>linewidths：标记取不同样式的时候，其中也许会有可以设置宽度的线，这时候这个参数是有效的。</li>
</ul>
<h2 id="优化器和学习率">3.3 优化器和学习率</h2>
<h3 id="优化器">3.3.1 优化器</h3>
<ul>
<li><strong>梯度下降 (Gradient Descent)</strong></li>
<li><strong>批量梯度下降 (Batch Gradient Descent)</strong>: 使用整个训练数据集来计算梯度。</li>
<li><strong>随机梯度下降 (Stochastic Gradient Descent, SGD)</strong>: 在每次迭代中使用一个训练样本来计算梯度。</li>
<li><strong>小批量梯度下降 (Mini-Batch Gradient Descent)</strong>: 使用一个小的数据子集（小批量）来计算梯度。2. <strong>Momentum</strong></li>
<li>为了加速SGD，Momentum考虑了过去的梯度来平滑更新。3. <strong>Adagrad (Adaptive Gradient Algorithm)</strong></li>
<li>为每个参数调整学习率，使频繁出现的参数有较小的学习率，而较少出现的参数有较大的学习率。4. <strong>RMSprop (Root Mean Square Propagation)</strong></li>
<li>与Adagrad类似，但是使用移动平均的平方梯度来调整学习率。5. <strong>Adam (Adaptive Moment Estimation)</strong></li>
<li>结合了Momentum和RMSprop的想法。它计算梯度的指数移动平均值和平方梯度的指数移动平均值来调整每个参数的学习率。6. <strong>Adadelta</strong></li>
<li>是Adagrad的一个扩展，旨在减少其学习率的急剧下降。7. <strong>Nadam</strong></li>
<li>结合了Adam和Nesterov的动量。8. <strong>FTRL (Follow The Regularized Leader)</strong></li>
<li>主要用于大规模线性模型，如LR和FM。9. <strong>L-BFGS (Limited-memory Broyden–Fletcher–Goldfarb–Shanno)</strong></li>
<li>一种准牛顿方法，通常用于全批量优化。</li>
</ul>
<p>每种优化器都有其特定的应用场景和优势。在深度学习中，Adam、RMSprop和SGD with Momentum是最常用的优化器。选择哪种优化器取决于具体的问题和数据。实际上，为了找到最佳的优化器和超参数设置，通常需要进行多次实验。</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/KGzhang/article/details/77479737">https://blog.csdn.net/KGzhang/article/details/77479737</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/Decennie/article/details/119222319">https://blog.csdn.net/Decennie/article/details/119222319</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/38945390">https://zhuanlan.zhihu.com/p/38945390</a></p>
<p>AdamW是一种深度学习模型的优化器，它基于Adam优化器（自适应矩估计）进行修改，更准确地实现了权重衰减。AdamW在一些情况下被观察到比原始的Adam优化器有更好的性能。</p>
<p>在深度学习中，我们通常通过随机梯度下降（SGD）或其变种来优化模型的参数，以最小化损失函数。这种优化过程可能会导致模型的参数值过大，从而导致过拟合。为了避免过拟合，我们可以对模型的参数进行权重衰减，即在优化过程中对模型参数应用一种惩罚。</p>
<p>原始的Adam优化器在其参数更新公式中包含一个衰减项，但这种方式实际上并不能有效地实现权重衰减。相反，它更类似于L2正则化，这与权重衰减的概念是有区别的。</p>
<p>在这里，AdamW优化器的提出就是为了解决这个问题。它将权重衰减与优化步骤分开，确保权重衰减只应用于模型的参数，而不是应用于梯度或者其它中间步骤。这样可以更精确地实现权重衰减，帮助模型在训练过程中保持更好的泛化能力。</p>
<p>总的来说，AdamW优化器在实践中通常可以带来更好的性能，尤其是对于大型的预训练模型，如BERT、GPT等。</p>
<h3 id="学习率">3.3.2 学习率</h3>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_35091353/article/details/117322293">待预热的学习率</a>。</p>
<p><code>get_cosine_schedule_with_warmup</code>：创建一个余弦退火学习率调度器，并包含了一个预热阶段。在预热阶段，学习率从 0 线性增加到初始学习率。预热步数由 <code>config.lr_warmup_steps</code> 指定。预热阶段过后，学习率开始按照余弦退火的方式下降。<code>num_training_steps</code>表示总的训练步数，计算方式是训练数据的批次数乘以训练的总周期数。<code>num_training_steps=(len(train_dataloader) * config.num_epochs)</code> 中，计算方式是训练数据的批次数乘以训练的总周期数。</p>

  
  
    
    <div class='footer'>
      
      
      
        <div class='copyright'>
          <blockquote>
            
              
                <p>博客内容遵循 署名-非商业性使用-相同方式共享 4.0 国际 (CC BY-NC-SA 4.0) 协议</p>

              
            
              
                <p>本文永久链接是：<a href=https://shymuel.top/3212441176.html>https://shymuel.top/3212441176.html</a></p>
              
            
          </blockquote>
        </div>
      
      
    </div>
  
  
    


  <div class='article-meta' id="bottom">
    <div class='new-meta-box'>
      
        
          <div class="new-meta-item date" itemprop="dateUpdated" datetime="2024-01-20T00:29:11+08:00">
  <a class='notlink'>
    <i class="fas fa-edit fa-fw" aria-hidden="true"></i>
    <p>更新于：2024年1月20日</p>
  </a>
</div>

        
      
        
          
  
  <div class="new-meta-item meta-tags"><a class="tag" href="/tags/AI%E6%A1%86%E6%9E%B6/" rel="nofollow"><i class="fas fa-hashtag fa-fw" aria-hidden="true"></i><p>AI框架</p></a></div> <div class="new-meta-item meta-tags"><a class="tag" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="nofollow"><i class="fas fa-hashtag fa-fw" aria-hidden="true"></i><p>机器学习</p></a></div> <div class="new-meta-item meta-tags"><a class="tag" href="/tags/Pytorch/" rel="nofollow"><i class="fas fa-hashtag fa-fw" aria-hidden="true"></i><p>Pytorch</p></a></div>


        
      
        
          
  <div class="new-meta-item share -mob-share-list">
  <div class="-mob-share-list share-body">
    
      
        <a class="-mob-share-qq" title="" rel="external nofollow noopener noreferrer noopener"
          
          target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=https://shymuel.top/3212441176.html&title=【竹夭的Pytorch讲义】01 - Shymuel's Blog&summary="
          
          >
          
            <img src="https://cdn.jsdelivr.net/gh/volantis-x/cdn-org/logo/128/qq.png" class="lazyload" data-srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-org/logo/128/qq.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=">
          
        </a>
      
    
      
        <a class="-mob-share-qzone" title="" rel="external nofollow noopener noreferrer noopener"
          
          target="_blank" href="https://sns.qzone.qq.com/cgi-bin/qzshare/cgi_qzshare_onekey?url=https://shymuel.top/3212441176.html&title=【竹夭的Pytorch讲义】01 - Shymuel's Blog&summary="
          
          >
          
            <img src="https://cdn.jsdelivr.net/gh/volantis-x/cdn-org/logo/128/qzone.png" class="lazyload" data-srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-org/logo/128/qzone.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=">
          
        </a>
      
    
      
        <a class="-mob-share-weibo" title="" rel="external nofollow noopener noreferrer noopener"
          
          target="_blank" href="http://service.weibo.com/share/share.php?url=https://shymuel.top/3212441176.html&title=【竹夭的Pytorch讲义】01 - Shymuel's Blog&summary="
          
          >
          
            <img src="https://cdn.jsdelivr.net/gh/volantis-x/cdn-org/logo/128/weibo.png" class="lazyload" data-srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-org/logo/128/weibo.png" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=">
          
        </a>
      
    
      
    
      
    
  </div>
</div>



        
      
    </div>
  </div>


  
  

  
    <div class="prev-next">
      
        <a class='prev' href='/415146091.html'>
          <p class='title'><i class="fas fa-chevron-left" aria-hidden="true"></i>【XAI】Explainable AI Resources</p>
          <p class='content'>本文总结了一系列XAI资源。可解释性技术根据算法用途的不同可分为：（1）输出解释Output Explanation，和（2）DNN原理解释Principle Explanation。根据研究对...</p>
        </a>
      
      
        <a class='next' href='/4240470103.html'>
          <p class='title'>【Haskell教程】01<i class="fas fa-chevron-right" aria-hidden="true"></i></p>
          <p class='content'>该系列博客是我结合多个来源的Haskell教程和自己对Haskell的理解写的。主体内容来源于w3cschool的Haskell教程，不过这个教程也是根据外国的一本教材翻译来的，我把一些语法错误...</p>
        </a>
      
    </div>
  
</article>


  

  <article class="post white-box reveal shadow" id="comments">
    <p ct><i class='fas fa-comments'></i> 评论</p>
    
    <div id="twikoo_container">
  <i class="fas fa-cog fa-spin fa-fw fa-2x"></i>
</div>

  </article>






</div>
<aside class='l_side'>
  
  
    
    



  <section class="widget toc-wrapper shadow desktop mobile" id="toc-div" >
    
  <header>
    
      <i class="fas fa-list fa-fw" aria-hidden="true"></i><span class='name'>本文目录</span>
    
  </header>


    <div class='content'>
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%85%8D%E7%BD%AE%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83"><span class="toc-text">1.1 配置开发环境</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9D%E8%AF%86pytorch"><span class="toc-text">1.2 初识pytorch</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%A4%E4%B8%AA%E5%AE%9E%E7%94%A8%E5%87%BD%E6%95%B0"><span class="toc-text">1.2.1 两个实用函数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E7%9A%84%E6%A6%82%E5%BF%B5%E5%92%8C%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C"><span class="toc-text">2.1 张量的概念和基本操作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E7%9A%84%E5%88%9B%E5%BB%BA%E5%92%8C%E7%B4%A2%E5%BC%95"><span class="toc-text">2.2 张量的创建和索引</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E7%9A%84%E5%88%9B%E5%BB%BA"><span class="toc-text">2.2.1 张量的创建</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E7%9A%84%E7%B4%A2%E5%BC%95"><span class="toc-text">2.2.2 张量的索引</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E7%9A%84%E7%BB%B4%E5%BA%A6%E6%93%8D%E4%BD%9C%E5%92%8C%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97"><span class="toc-text">2.3 张量的维度操作和数学运算</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E7%9A%84%E7%BB%B4%E5%BA%A6%E6%93%8D%E4%BD%9C"><span class="toc-text">2.3.1 张量的维度操作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E7%9A%84%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97"><span class="toc-text">2.3.2 张量的数学运算</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#section"><span class="toc-text"></span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E5%9B%BE%E4%B8%8E%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC"><span class="toc-text">3.1 计算图与自动求导</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8pytorch%E8%BF%9B%E8%A1%8C%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-text">3.2 使用PyTorch进行自动求导和反向传播</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E5%99%A8%E5%92%8C%E5%AD%A6%E4%B9%A0%E7%8E%87"><span class="toc-text">3.3 优化器和学习率</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-text">3.3.1 优化器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AD%A6%E4%B9%A0%E7%8E%87"><span class="toc-text">3.3.2 学习率</span></a></li></ol></li></ol>
    </div>
  </section>


  


</aside>



		  
		  <!--此文件用来存放一些不方便取值的变量-->
<!--思路大概是将值藏到重加载的区域内-->

<script>
  window.pdata={}
  pdata.ispage=true;
  pdata.postTitle="【竹夭的Pytorch讲义】01";
  pdata.commentPath="";
  pdata.commentPlaceholder="";
  // header 这里无论是否开启pjax都需要
  var l_header=document.getElementById("l_header");
  
  l_header.classList.add("show");
  
  
    // cover
    var cover_wrapper=document.querySelector('.cover-wrapper');
    
    cover_wrapper.id="none";
    cover_wrapper.style.display="none";
    
  
</script>

        </div>
        
  
  <footer class="footer clearfix">
    <br><br>
    
      
        <div class="aplayer-container">
          


        </div>
      
    
      
        <br>
        <div class="social-wrapper">
          
            
          
            
          
            
          
        </div>
      
    
      
        <div><p>博客内容遵循 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">署名-非商业性使用-相同方式共享 4.0 国际 (CC BY-NC-SA 4.0) 协议</a></p>
</div>
      
    
      
        
          <div><p><span id="lc-sv">本站总访问量为 <span id="number"><i class="fas fa-circle-notch fa-spin fa-fw" aria-hidden="true"></i></span> 次</span> <span id="lc-uv">访客数为 <span id="number"><i class="fas fa-circle-notch fa-spin fa-fw" aria-hidden="true"></i></span> 人</span></p>
</div>
        
      
    
      
        本站使用
        <a href="https://github.com/volantis-x/hexo-theme-volantis/tree/4.3.1" target="_blank" class="codename">Volantis</a>
        作为主题
      
    
      
        <div class='copyright'>
        <p><a href="/">Copyright © 2021-2021 Shymuel</a></p>

        </div>
      
    
  </footer>


        <a id="s-top" class="fas fa-arrow-up fa-fw" href="javascript:void(0)"></a>
      </div>
    </div>
    <div>
      <script>
/************这个文件存放不需要重载的全局变量和全局函数*********/
window.volantis={};
window.volantis.loadcss=document.getElementById("loadcss");
/******************** Pjax ********************************/
function VPjax(){
	this.list=[] // 存放回调函数
	this.start=()=>{
	  for(var i=0;i<this.list.length;i++){
		this.list[i].run();
	  }
	}
	this.push=(fn,name)=>{
		var f=new PjaxItem(fn,name);
		this.list.push(f);
	}
	// 构造一个可以run的对象
	function PjaxItem(fn,name){
		// 函数名称
		this.name = name || fn.name
		// run方法
		this.run=()=>{
			fn()
		}
	}
}
volantis.pjax={}
volantis.pjax.method={
	complete: new VPjax(),
	error: new VPjax(),
	send: new VPjax()
}
volantis.pjax={
	...volantis.pjax,
	push: volantis.pjax.method.complete.push,
	error: volantis.pjax.method.error.push,
	send: volantis.pjax.method.send.push
}
/********************脚本懒加载函数********************************/
// 已经加入了setTimeout
function loadScript(src, cb) {
	setTimeout(function() {
		var HEAD = document.getElementsByTagName('head')[0] || document.documentElement;
		var script = document.createElement('script');
		script.setAttribute('type','text/javascript');
		if (cb) script.onload = cb;
		script.setAttribute('src', src);
		HEAD.appendChild(script);
	});
}
//https://github.com/filamentgroup/loadCSS
var loadCSS = function( href, before, media, attributes ){
	var doc = window.document;
	var ss = doc.createElement( "link" );
	var ref;
	if( before ){
		ref = before;
	}
	else {
		var refs = ( doc.body || doc.getElementsByTagName( "head" )[ 0 ] ).childNodes;
		ref = refs[ refs.length - 1];
	}
	var sheets = doc.styleSheets;
	if( attributes ){
		for( var attributeName in attributes ){
			if( attributes.hasOwnProperty( attributeName ) ){
				ss.setAttribute( attributeName, attributes[attributeName] );
			}
		}
	}
	ss.rel = "stylesheet";
	ss.href = href;
	ss.media = "only x";
	function ready( cb ){
		if( doc.body ){
			return cb();
		}
		setTimeout(function(){
			ready( cb );
		});
	}
	ready( function(){
		ref.parentNode.insertBefore( ss, ( before ? ref : ref.nextSibling ) );
	});
	var onloadcssdefined = function( cb ){
		var resolvedHref = ss.href;
		var i = sheets.length;
		while( i-- ){
			if( sheets[ i ].href === resolvedHref ){
				return cb();
			}
		}
		setTimeout(function() {
			onloadcssdefined( cb );
		});
	};
	function loadCB(){
		if( ss.addEventListener ){
			ss.removeEventListener( "load", loadCB );
		}
		ss.media = media || "all";
	}
	if( ss.addEventListener ){
		ss.addEventListener( "load", loadCB);
	}
	ss.onloadcssdefined = onloadcssdefined;
	onloadcssdefined( loadCB );
	return ss;
};
</script>
<script>
  
  loadCSS("https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.14/css/all.min.css", window.volantis.loadcss);
  
  
  
  
</script>
<!-- required -->

<script src="https://cdn.jsdelivr.net/npm/jquery@3.5/dist/jquery.min.js"></script>

<script>
  function pjax_fancybox() {
    $(".md .gallery").find("img").each(function () { //渲染 fancybox
      var element = document.createElement("a"); // a 标签
      $(element).attr("class", "fancybox");
      $(element).attr("pjax-fancybox", "");  // 过滤 pjax
      $(element).attr("href", $(this).attr("src"));
      if ($(this).attr("data-original")) {
        $(element).attr("href", $(this).attr("data-original"));
      }
      $(element).attr("data-fancybox", "images");
      var caption = "";   // 描述信息
      if ($(this).attr('alt')) {  // 判断当前页面是否存在描述信息
        $(element).attr('data-caption', $(this).attr('alt'));
        caption = $(this).attr('alt');
      }
      var div = document.createElement("div");
      $(div).addClass("fancybox");
      $(this).wrap(div); // 最外层套 div ，其实主要作用还是 class 样式
      var span = document.createElement("span");
      $(span).addClass("image-caption");
      $(span).text(caption); // 加描述
      $(this).after(span);  // 再套一层描述
      $(this).wrap(element);  // 最后套 a 标签
    })
    $(".md .gallery").find("img").fancybox({
      selector: '[data-fancybox="images"]',
      hash: false,
      loop: false,
      closeClick: true,
      helpers: {
        overlay: {closeClick: true}
      },
      buttons: [
        "zoom",
        "close"
      ]
    });
  };
  function SCload_fancybox() {
    if ($(".md .gallery").find("img").length == 0) return;
    loadCSS("https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css", document.getElementById("loadcss"));
    loadScript('https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js', pjax_fancybox)
  };
  $(function () {
    SCload_fancybox();
  });
  function Pjax_SCload_fancybox(){
	if (typeof $.fancybox == "undefined") {
	 SCload_fancybox();
    } else {
	 pjax_fancybox();
    }
  }
  volantis.pjax.push(Pjax_SCload_fancybox)
  volantis.pjax.send(()=>{
      if (typeof $.fancybox != "undefined") {
        $.fancybox.close();    // 关闭弹窗
      }
  },'fancybox')
</script>


<!-- internal -->




<script>
  function loadIssuesJS() {
    if ($(".md").find(".issues-api").length == 0) return;
	
	  loadScript('/js/issues.js');
	
  };
  $(function () {
    loadIssuesJS();
  });
  volantis.pjax.push(()=>{
	if (typeof IssuesAPI == "undefined") {
	  loadIssuesJS();
	}
  },"IssuesJS")
</script>



  <script defer src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.1.0/dist/lazyload.min.js"></script>
<script>
  // https://www.npmjs.com/package/vanilla-lazyload
  // Set the options globally
  // to make LazyLoad self-initialize
  window.lazyLoadOptions = {
    elements_selector: ".lazyload",
    threshold: 0
  };
  // Listen to the initialization event
  // and get the instance of LazyLoad
  window.addEventListener(
    "LazyLoad::Initialized",
    function (event) {
      window.lazyLoadInstance = event.detail.instance;
    },
    false
  );
  document.addEventListener('DOMContentLoaded', function () {
    lazyLoadInstance.update();
  });
  document.addEventListener('pjax:complete', function () {
    lazyLoadInstance.update();
  });
</script>




  

<script>
  window.FPConfig = {
	delay: 0,
	ignoreKeywords: [],
	maxRPS: 5,
	hoverDelay: 25
  };
</script>
<script defer src="https://cdn.jsdelivr.net/gh/gijo-varghese/flying-pages@2.1.2/flying-pages.min.js"></script>











  <script>
  function load_twikoo() {
	
	  loadScript('https://cdn.jsdelivr.net/npm/twikoo@latest', pjax_twikoo)
	
  }
  function pjax_twikoo() {
    if(!document.querySelectorAll("#twikoo_container")[0])return;
    let path = pdata.commentPath;
    if (path.length == 0) {
      let defaultPath = '';
      path = defaultPath || "decodeURI(window.location.pathname)"; //神奇的pathname
    }
    twikoo.init(Object.assign({"js":"https://cdn.jsdelivr.net/npm/twikoo@latest","path":null,"envId":"https://twikoo-dusky.vercel.app/","placeholder":"快来评论吧~","appId":null,"appKey":null,"meta":["nick","mail","link"],"requiredFields":["nick","mail"],"enableQQ":true,"recordIP":false,"avatar":"/img/avatar.jpg","pageSize":10,"lang":"zh-cn","highlight":true,"mathJax":true}, {
	  el: '#twikoo_container',
	  path: path,
    }))
  }
  $(function () {
    if(!document.querySelectorAll("#twikoo_container")[0])return;
    load_twikoo();
  });
   volantis.pjax.push(()=>{
	  if (typeof twikoo == "undefined") {
		load_twikoo();
	  } else {
		pjax_twikoo();
	  }
   },'twikoo');
</script>





  
<script src="/js/app.js"></script>



<!-- optional -->

  <script>
const SearchServiceimagePath="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@master/img/";
const ROOT =  ("/" || "/").endsWith('/') ? ("/" || "/") : ("//" || "/" );

$('.input.u-search-input').one('focus',function(){
	
		loadScript('/js/search/hexo.js',setSearchService);
	
})

function listenSearch(){
  
    customSearch = new HexoSearch({
      imagePath: SearchServiceimagePath
    });
  
}
function setSearchService() {
	listenSearch();
	
		document.addEventListener("pjax:success", listenSearch);
	
}
</script>











  <script defer>

  const LCCounter = {
    app_id: 'u9j57bwJod4EDmXWdxrwuqQT-MdYXbMMI',
    app_key: 'jfHtEKVE24j0IVCGHbvuFClp',
    custom_api_server: '',

    // 查询存储的记录
    getRecord(Counter, url, title) {
      return new Promise(function (resolve, reject) {
        Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({url})))
          .then(resp => resp.json())
          .then(({results, code, error}) => {
            if (code === 401) {
              throw error;
            }
            if (results && results.length > 0) {
              var record = results[0];
              resolve(record);
            } else {
              Counter('post', '/classes/Counter', {url, title: title, times: 0})
                .then(resp => resp.json())
                .then((record, error) => {
                  if (error) {
                    throw error;
                  }
                  resolve(record);
                }).catch(error => {
                console.error('Failed to create', error);
                reject(error);
              });
            }
          }).catch((error) => {
          console.error('LeanCloud Counter Error:', error);
          reject(error);
        });
      })
    },

    // 发起自增请求
    increment(Counter, incrArr) {
      return new Promise(function (resolve, reject) {
        Counter('post', '/batch', {
          "requests": incrArr
        }).then((res) => {
          res = res.json();
          if (res.error) {
            throw res.error;
          }
          resolve(res);
        }).catch((error) => {
          console.error('Failed to save visitor count', error);
          reject(error);
        });
      });
    },

    // 构建自增请求体
    buildIncrement(objectId) {
      return {
        "method": "PUT",
        "path": `/1.1/classes/Counter/${ objectId }`,
        "body": {
          "times": {
            '__op': 'Increment',
            'amount': 1
          }
        }
      }
    },

    // 校验是否为有效的 UV
    validUV() {
      var key = 'LeanCloudUVTimestamp';
      var flag = localStorage.getItem(key);
      if (flag) {
        // 距离标记小于 24 小时则不计为 UV
        if (new Date().getTime() - parseInt(flag) <= 86400000) {
          return false;
        }
      }
      localStorage.setItem(key, new Date().getTime().toString());
      return true;
    },

    addCount(Counter) {
      var enableIncr = '' === 'true' && window.location.hostname !== 'localhost';
      enableIncr = true;
      var getterArr = [];
      var incrArr = [];
      // 请求 PV 并自增
      var pvCtn = document.querySelector('#lc-sv');
      if (pvCtn || enableIncr) {
        var pvGetter = this.getRecord(Counter, 'https://shymuel.top' + '/#lc-sv', 'Visits').then((record) => {
          incrArr.push(this.buildIncrement(record.objectId))
          var eles = document.querySelectorAll('#lc-sv #number');
          if (eles.length > 0) {
            eles.forEach((el,index,array)=>{
              el.innerText = record.times + 1;
              if (pvCtn) {
                pvCtn.style.display = 'inline';
              }
            })
          }
        });
        getterArr.push(pvGetter);
      }

      // 请求 UV 并自增
      var uvCtn = document.querySelector('#lc-uv');
      if (uvCtn || enableIncr) {
        var uvGetter = this.getRecord(Counter, 'https://shymuel.top' + '/#lc-uv', 'Visitors').then((record) => {
          var vuv = this.validUV();
          vuv && incrArr.push(this.buildIncrement(record.objectId))
          var eles = document.querySelectorAll('#lc-uv #number');
          if (eles.length > 0) {
            eles.forEach((el,index,array)=>{
              el.innerText = record.times + (vuv ? 1 : 0);
              if (uvCtn) {
                uvCtn.style.display = 'inline';
              }
            })
          }
        });
        getterArr.push(uvGetter);
      }

      // 请求文章的浏览数，如果是当前页面就自增
      var allPV = document.querySelectorAll('#lc-pv');
      if (allPV.length > 0 || enableIncr) {
        for (i = 0; i < allPV.length; i++) {
          let pv = allPV[i];
          let title = pv.getAttribute('data-title');
          var url = 'https://shymuel.top' + pv.getAttribute('data-path');
          if (url) {
            var viewGetter = this.getRecord(Counter, url, title).then((record) => {
              // 是当前页面就自增
              let curPath = window.location.pathname;
              if (curPath.includes('index.html')) {
                curPath = curPath.substring(0, curPath.lastIndexOf('index.html'));
              }
              if (pv.getAttribute('data-path') == curPath) {
                incrArr.push(this.buildIncrement(record.objectId));
              }
              if (pv) {
                var ele = pv.querySelector('#lc-pv #number');
                if (ele) {
                  if (pv.getAttribute('data-path') == curPath) {
                    ele.innerText = (record.times || 0) + 1;
                  } else {
                    ele.innerText = record.times || 0;
                  }
                  pv.style.display = 'inline';
                }
              }
            });
            getterArr.push(viewGetter);
          }
        }
      }

      // 如果启动计数自增，批量发起自增请求
      if (enableIncr) {
        Promise.all(getterArr).then(() => {
          incrArr.length > 0 && this.increment(Counter, incrArr);
        })
      }

    },


    fetchData(api_server) {
      var Counter = (method, url, data) => {
        return fetch(`${ api_server }/1.1${ url }`, {
          method,
          headers: {
            'X-LC-Id': this.app_id,
            'X-LC-Key': this.app_key,
            'Content-Type': 'application/json',
          },
          body: JSON.stringify(data)
        });
      };
      this.addCount(Counter);
    },

    refreshCounter() {
      var api_server = this.app_id.slice(-9) !== '-MdYXbMMI' ? this.custom_api_server : `https://${ this.app_id.slice(0, 8).toLowerCase() }.api.lncldglobal.com`;
      if (api_server) {
        this.fetchData(api_server);
      } else {
        fetch('https://app-router.leancloud.cn/2/route?appId=' + this.app_id)
          .then(resp => resp.json())
          .then(({api_server}) => {
            this.fetchData('https://' + api_server);
          });
      }
    }

  };

  LCCounter.refreshCounter();

  document.addEventListener('pjax:complete', function () {
    LCCounter.refreshCounter();
  });
</script>




  <script>
const rootElement = document.documentElement;
const darkModeStorageKey = "user-color-scheme";
const rootElementDarkModeAttributeName = "data-user-color-scheme";

const setLS = (k, v) => {
    localStorage.setItem(k, v);
};

const removeLS = (k) => {
    localStorage.removeItem(k);
};

const getLS = (k) => {
    return localStorage.getItem(k);
};

const getModeFromCSSMediaQuery = () => {
  return window.matchMedia("(prefers-color-scheme: dark)").matches
    ? "dark"
    : "light";
};

const resetRootDarkModeAttributeAndLS = () => {
  rootElement.removeAttribute(rootElementDarkModeAttributeName);
  removeLS(darkModeStorageKey);
};

const validColorModeKeys = {
  dark: true,
  light: true,
};

const applyCustomDarkModeSettings = (mode) => {
  const currentSetting = mode || getLS(darkModeStorageKey);

  if (currentSetting === getModeFromCSSMediaQuery()) {
    resetRootDarkModeAttributeAndLS();
  } else if (validColorModeKeys[currentSetting]) {
    rootElement.setAttribute(rootElementDarkModeAttributeName, currentSetting);
  } else {
    resetRootDarkModeAttributeAndLS();
  }
};

const invertDarkModeObj = {
  dark: "light",
  light: "dark",
};

/**
 * get target mode
 */
const toggleCustomDarkMode = () => {
  let currentSetting = getLS(darkModeStorageKey);

  if (validColorModeKeys[currentSetting]) {
    currentSetting = invertDarkModeObj[currentSetting];
  } else if (currentSetting === null) {
    currentSetting = invertDarkModeObj[getModeFromCSSMediaQuery()];
  } else {
    return;
  }
  setLS(darkModeStorageKey, currentSetting);
  return currentSetting;
};

/**
 * bind click event for toggle button
 */
var btn=$("#wrapper .toggle-mode-btn,#rightmenu-wrapper .toggle-mode-btn");
function bindToggleButton() {
    btn.on('click',(e) => {
      const mode = toggleCustomDarkMode();
      applyCustomDarkModeSettings(mode);
    });
}

applyCustomDarkModeSettings();
document.addEventListener("DOMContentLoaded", bindToggleButton);
volantis.pjax.push(bindToggleButton);
volantis.pjax.send(()=>{
	btn.unbind('click');
},'toggle-mode-btn-unbind');
</script>








<script>
function listennSidebarTOC() {
  const navItems = document.querySelectorAll(".toc li");
  if (!navItems.length) return;
  const sections = [...navItems].map((element) => {
    const link = element.querySelector(".toc-link");
    const target = document.getElementById(
      decodeURI(link.getAttribute("href")).replace("#", "")
    );
    link.addEventListener("click", (event) => {
      event.preventDefault();
      window.scrollTo({
		top: target.offsetTop + 100,
		
		behavior: "smooth"
		
	  });
    });
    return target;
  });

  function activateNavByIndex(target) {
    if (target.classList.contains("active-current")) return;

    document.querySelectorAll(".toc .active").forEach((element) => {
      element.classList.remove("active", "active-current");
    });
    target.classList.add("active", "active-current");
    let parent = target.parentNode;
    while (!parent.matches(".toc")) {
      if (parent.matches("li")) parent.classList.add("active");
      parent = parent.parentNode;
    }
  }

  function findIndex(entries) {
    let index = 0;
    let entry = entries[index];
    if (entry.boundingClientRect.top > 0) {
      index = sections.indexOf(entry.target);
      return index === 0 ? 0 : index - 1;
    }
    for (; index < entries.length; index++) {
      if (entries[index].boundingClientRect.top <= 0) {
        entry = entries[index];
      } else {
        return sections.indexOf(entry.target);
      }
    }
    return sections.indexOf(entry.target);
  }

  function createIntersectionObserver(marginTop) {
    marginTop = Math.floor(marginTop + 10000);
    let intersectionObserver = new IntersectionObserver(
      (entries, observe) => {
        let scrollHeight = document.documentElement.scrollHeight + 100;
        if (scrollHeight > marginTop) {
          observe.disconnect();
          createIntersectionObserver(scrollHeight);
          return;
        }
        let index = findIndex(entries);
        activateNavByIndex(navItems[index]);
      },
      {
        rootMargin: marginTop + "px 0px -100% 0px",
        threshold: 0,
      }
    );
    sections.forEach((element) => {
      element && intersectionObserver.observe(element);
    });
  }
  createIntersectionObserver(document.documentElement.scrollHeight);
}

document.addEventListener("DOMContentLoaded", listennSidebarTOC);
document.addEventListener("pjax:success", listennSidebarTOC);
</script>

<!-- more -->

 
	   
	    


<script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.min.js"></script>


<script>
    var pjax;
    document.addEventListener('DOMContentLoaded', function () {
      pjax = new Pjax({
        elements: 'a[href]:not([href^="#"]):not([href="javascript:void(0)"]):not([pjax-fancybox])',
        selectors: [
          "title",
          
          "#pjax-container",
          "#pjax-header-nav-list"
        ],
        cacheBust: false,   // url 地址追加时间戳，用以避免浏览器缓存
        timeout: 5000
      });
    });

    document.addEventListener('pjax:send', function (e) {
      //window.stop(); // 相当于点击了浏览器的停止按钮

      try {
        var currentUrl = window.location.pathname;
        var targetUrl = e.triggerElement.href;
        var banUrl = [""];
        if (banUrl[0] != "") {
          banUrl.forEach(item => {
            if(currentUrl.indexOf(item) != -1 || targetUrl.indexOf(item) != -1) {
              window.location.href = targetUrl;
            }
          });
        }
      } catch (error) {}

      window.subData = null; // 移除标题（用于一二级导航栏切换处）

      volantis.$switcher.removeClass('active'); // 关闭移动端激活的搜索框
      volantis.$header.removeClass('z_search-open'); // 关闭移动端激活的搜索框
      volantis.$wrapper.removeClass('sub'); // 跳转页面时关闭二级导航

      // 解绑事件 避免重复监听
      volantis.$topBtn.unbind('click');
      $('.menu a').unbind('click');
      $(window).unbind('resize');
      $(window).unbind('scroll');
      $(document).unbind('scroll');
      $(document).unbind('click');
      $('body').unbind('click');
	  // 使用 volantis.pjax.send 方法传入pjax:send回调函数 参见layout/_partial/scripts/global.ejs
	  volantis.pjax.method.send.start();
    });

    document.addEventListener('pjax:complete', function () {
      $('.nav-main').find('.list-v').not('.menu-phone').removeAttr("style",""); // 移除小尾巴的移除
      $('.menu-phone.list-v').removeAttr("style",""); // 移除小尾巴的移除
      $('script[data-pjax], .pjax-reload script').each(function () {
        $(this).parent().append($(this).remove());
      });
      try{
		// 使用 volantis.pjax.push 方法传入重载函数 参见layout/_partial/scripts/global.ejs
		volantis.pjax.method.complete.start();
      } catch (e) {
        console.log(e);
      }
    });

    document.addEventListener('pjax:error', function (e) {
	  // 使用 volantis.pjax.error 方法传入pjax:error回调函数 参见layout/_partial/scripts/global.ejs
	  volantis.pjax.method.error.start();
      window.location.href = e.triggerElement.href;
    });
</script>
 
	  
    </div>
  </body>
</html>
